"""
Power User Tools Module
Advanced features including undo/redo, code generation, and learning systems
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import json
import copy


@dataclass
class DataSnapshot:
    """Represents a snapshot of the dataset at a point in time"""
    timestamp: str
    name: str
    data: pd.DataFrame
    profile: Dict[str, Any]
    operation: str
    description: str
    
    def to_dict(self, include_data: bool = False):
        """Convert to dictionary"""
        result = {
            'timestamp': self.timestamp,
            'name': self.name,
            'operation': self.operation,
            'description': self.description,
            'rows': len(self.data),
            'columns': len(self.data.columns),
            'memory_mb': self.data.memory_usage(deep=True).sum() / 1024**2
        }
        if include_data:
            result['data'] = self.data.to_dict('records')
        return result


class SnapshotManager:
    """Manages dataset snapshots for undo/redo functionality"""
    
    def __init__(self, max_snapshots: int = 10):
        self.snapshots: List[DataSnapshot] = []
        self.current_index: int = -1
        self.max_snapshots = max_snapshots
    
    def save_snapshot(self, df: pd.DataFrame, profile: Dict[str, Any],
                     operation: str, description: str) -> DataSnapshot:
        """Save a new snapshot"""
        # Remove any snapshots after current index (they're invalidated by new action)
        if self.current_index < len(self.snapshots) - 1:
            self.snapshots = self.snapshots[:self.current_index + 1]
        
        # Create snapshot
        snapshot = DataSnapshot(
            timestamp=datetime.now().isoformat(),
            name=f"Snapshot_{len(self.snapshots) + 1}",
            data=df.copy(),
            profile=copy.deepcopy(profile),
            operation=operation,
            description=description
        )
        
        # Add to list
        self.snapshots.append(snapshot)
        
        # Maintain max snapshots limit
        if len(self.snapshots) > self.max_snapshots:
            self.snapshots.pop(0)
        else:
            self.current_index += 1
        
        return snapshot
    
    def undo(self) -> Optional[DataSnapshot]:
        """Undo to previous snapshot"""
        if not self.can_undo():
            return None
        
        self.current_index -= 1
        return self.snapshots[self.current_index]
    
    def redo(self) -> Optional[DataSnapshot]:
        """Redo to next snapshot"""
        if not self.can_redo():
            return None
        
        self.current_index += 1
        return self.snapshots[self.current_index]
    
    def can_undo(self) -> bool:
        """Check if undo is possible"""
        return self.current_index > 0
    
    def can_redo(self) -> bool:
        """Check if redo is possible"""
        return self.current_index < len(self.snapshots) - 1
    
    def get_current_snapshot(self) -> Optional[DataSnapshot]:
        """Get current snapshot"""
        if 0 <= self.current_index < len(self.snapshots):
            return self.snapshots[self.current_index]
        return None
    
    def get_history(self) -> List[Dict[str, Any]]:
        """Get snapshot history"""
        return [
            {
                **snapshot.to_dict(),
                'is_current': i == self.current_index
            }
            for i, snapshot in enumerate(self.snapshots)
        ]
    
    def clear_history(self):
        """Clear all snapshots"""
        self.snapshots.clear()
        self.current_index = -1
    
    def jump_to_snapshot(self, index: int) -> Optional[DataSnapshot]:
        """Jump to specific snapshot"""
        if 0 <= index < len(self.snapshots):
            self.current_index = index
            return self.snapshots[index]
        return None


class CodeGenerator:
    """Generates executable code from cleaning operations"""
    
    def __init__(self):
        self.operations = []
    
    def add_operation(self, operation: str, parameters: Dict[str, Any]):
        """Add an operation to generate code for"""
        self.operations.append({
            'operation': operation,
            'parameters': parameters
        })
    
    def generate_pandas_code(self, df_name: str = "df") -> str:
        """Generate pandas/Python code"""
        code_lines = [
            "# Generated Data Cleaning Code",
            "# Generated by Data Cleaning System",
            f"# Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "import pandas as pd",
            "import numpy as np",
            "",
            f"# Load your data",
            f"# {df_name} = pd.read_csv('your_file.csv')",
            "",
            "# Data Cleaning Operations",
            ""
        ]
        
        for op in self.operations:
            operation = op['operation']
            params = op['parameters']
            
            if operation == 'remove_duplicates':
                code_lines.append(f"# Remove duplicate rows")
                code_lines.append(f"{df_name} = {df_name}.drop_duplicates()")
                code_lines.append("")
            
            elif operation == 'handle_missing':
                code_lines.append(f"# Handle missing values")
                for col, method in params.items():
                    if method == 'drop':
                        code_lines.append(f"{df_name} = {df_name}.dropna(subset=['{col}'])")
                    elif method == 'mean':
                        code_lines.append(f"{df_name}['{col}'].fillna({df_name}['{col}'].mean(), inplace=True)")
                    elif method == 'median':
                        code_lines.append(f"{df_name}['{col}'].fillna({df_name}['{col}'].median(), inplace=True)")
                    elif method == 'mode':
                        code_lines.append(f"{df_name}['{col}'].fillna({df_name}['{col}'].mode()[0], inplace=True)")
                    elif method == 'forward_fill':
                        code_lines.append(f"{df_name}['{col}'].fillna(method='ffill', inplace=True)")
                    elif method == 'backward_fill':
                        code_lines.append(f"{df_name}['{col}'].fillna(method='bfill', inplace=True)")
                    elif method.startswith('constant:'):
                        value = method.split(':', 1)[1]
                        code_lines.append(f"{df_name}['{col}'].fillna('{value}', inplace=True)")
                code_lines.append("")
            
            elif operation == 'type_conversion':
                code_lines.append(f"# Convert data types")
                for col, target_type in params.items():
                    if target_type == 'numeric':
                        code_lines.append(f"{df_name}['{col}'] = pd.to_numeric({df_name}['{col}'], errors='coerce')")
                    elif target_type == 'datetime':
                        code_lines.append(f"{df_name}['{col}'] = pd.to_datetime({df_name}['{col}'], errors='coerce')")
                    elif target_type == 'string':
                        code_lines.append(f"{df_name}['{col}'] = {df_name}['{col}'].astype(str)")
                    elif target_type == 'category':
                        code_lines.append(f"{df_name}['{col}'] = {df_name}['{col}'].astype('category')")
                code_lines.append("")
            
            elif operation == 'normalize_text':
                code_lines.append(f"# Normalize text columns")
                for col in params:
                    code_lines.append(f"{df_name}['{col}'] = {df_name}['{col}'].astype(str).str.strip()")
                    code_lines.append(f"{df_name}['{col}'] = {df_name}['{col}'].str.replace(r'\\s+', ' ', regex=True)")
                code_lines.append("")
            
            elif operation == 'handle_outliers':
                code_lines.append(f"# Handle outliers")
                for col, method in params.items():
                    code_lines.append(f"# Column: {col}")
                    code_lines.append(f"Q1 = {df_name}['{col}'].quantile(0.25)")
                    code_lines.append(f"Q3 = {df_name}['{col}'].quantile(0.75)")
                    code_lines.append(f"IQR = Q3 - Q1")
                    code_lines.append(f"lower_bound = Q1 - 1.5 * IQR")
                    code_lines.append(f"upper_bound = Q3 + 1.5 * IQR")
                    
                    if method == 'remove':
                        code_lines.append(f"{df_name} = {df_name}[({df_name}['{col}'] >= lower_bound) & ({df_name}['{col}'] <= upper_bound)]")
                    elif method == 'cap':
                        code_lines.append(f"{df_name}.loc[{df_name}['{col}'] < lower_bound, '{col}'] = lower_bound")
                        code_lines.append(f"{df_name}.loc[{df_name}['{col}'] > upper_bound, '{col}'] = upper_bound")
                    code_lines.append("")
            
            elif operation == 'remove_columns':
                code_lines.append(f"# Remove columns")
                cols = ", ".join([f"'{col}'" for col in params])
                code_lines.append(f"{df_name} = {df_name}.drop(columns=[{cols}])")
                code_lines.append("")
            
            elif operation == 'rename_columns':
                code_lines.append(f"# Rename columns")
                mapping = ", ".join([f"'{old}': '{new}'" for old, new in params.items()])
                code_lines.append(f"{df_name} = {df_name}.rename(columns={{{mapping}}})")
                code_lines.append("")
        
        code_lines.extend([
            "# Save cleaned data",
            f"# {df_name}.to_csv('cleaned_data.csv', index=False)",
            "",
            f"print(f'Cleaning complete! Shape: {{{df_name}.shape}}')"
        ])
        
        return "\n".join(code_lines)
    
    def generate_pyspark_code(self, df_name: str = "df") -> str:
        """Generate PySpark code"""
        code_lines = [
            "# Generated PySpark Data Cleaning Code",
            f"# Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "from pyspark.sql import SparkSession",
            "from pyspark.sql.functions import *",
            "from pyspark.sql.window import Window",
            "",
            "# Initialize Spark session",
            "spark = SparkSession.builder.appName('DataCleaning').getOrCreate()",
            "",
            f"# Load your data",
            f"# {df_name} = spark.read.csv('your_file.csv', header=True, inferSchema=True)",
            "",
            "# Data Cleaning Operations",
            ""
        ]
        
        for op in self.operations:
            operation = op['operation']
            params = op['parameters']
            
            if operation == 'remove_duplicates':
                code_lines.append(f"# Remove duplicate rows")
                code_lines.append(f"{df_name} = {df_name}.dropDuplicates()")
                code_lines.append("")
            
            elif operation == 'handle_missing':
                code_lines.append(f"# Handle missing values")
                for col, method in params.items():
                    if method == 'drop':
                        code_lines.append(f"{df_name} = {df_name}.dropna(subset=['{col}'])")
                    elif method in ['mean', 'median']:
                        code_lines.append(f"{col}_fill = {df_name}.select(avg('{col}')).first()[0]")
                        code_lines.append(f"{df_name} = {df_name}.fillna({{{col}_fill}}, subset=['{col}'])")
                    elif method.startswith('constant:'):
                        value = method.split(':', 1)[1]
                        code_lines.append(f"{df_name} = {df_name}.fillna({{'{col}': '{value}'}})")
                code_lines.append("")
            
            elif operation == 'type_conversion':
                code_lines.append(f"# Convert data types")
                for col, target_type in params.items():
                    if target_type == 'numeric':
                        code_lines.append(f"{df_name} = {df_name}.withColumn('{col}', col('{col}').cast('double'))")
                    elif target_type == 'datetime':
                        code_lines.append(f"{df_name} = {df_name}.withColumn('{col}', to_timestamp(col('{col}')))")
                    elif target_type == 'string':
                        code_lines.append(f"{df_name} = {df_name}.withColumn('{col}', col('{col}').cast('string'))")
                code_lines.append("")
            
            elif operation == 'normalize_text':
                code_lines.append(f"# Normalize text columns")
                for col in params:
                    code_lines.append(f"{df_name} = {df_name}.withColumn('{col}', trim(col('{col}')))")
                    code_lines.append(f"{df_name} = {df_name}.withColumn('{col}', regexp_replace(col('{col}'), '\\\\s+', ' '))")
                code_lines.append("")
        
        code_lines.extend([
            "# Save cleaned data",
            f"# {df_name}.write.csv('cleaned_data.csv', header=True, mode='overwrite')",
            "",
            f"print(f'Cleaning complete! Count: {{{df_name}.count()}}')"
        ])
        
        return "\n".join(code_lines)
    
    def generate_sql_code(self, table_name: str = "source_table") -> str:
        """Generate SQL code"""
        code_lines = [
            "-- Generated SQL Data Cleaning Code",
            f"-- Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "-- Create cleaned table",
            f"CREATE TABLE cleaned_{table_name} AS",
            "SELECT"
        ]
        
        # This is a simplified SQL generator - full SQL would be more complex
        code_lines.extend([
            "  *,",
            "  -- Add your transformations here",
            f"FROM {table_name}",
            ""
        ])
        
        for op in self.operations:
            operation = op['operation']
            
            if operation == 'remove_duplicates':
                code_lines.append("-- Remove duplicates using ROW_NUMBER()")
                code_lines.append("-- WHERE row_num = 1")
        
        return "\n".join(code_lines)
    
    def export_as_notebook(self, df_name: str = "df") -> Dict[str, Any]:
        """Export as Jupyter notebook format"""
        notebook = {
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "# Data Cleaning Notebook\n",
                        f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
                        "\n",
                        "This notebook contains the data cleaning operations performed by the Data Cleaning System."
                    ]
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Import libraries\n",
                        "import pandas as pd\n",
                        "import numpy as np\n",
                        "\n",
                        "# Load data\n",
                        f"# {df_name} = pd.read_csv('your_file.csv')"
                    ]
                }
            ],
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 4
        }
        
        # Add operation cells
        for op in self.operations:
            operation = op['operation']
            params = op['parameters']
            
            markdown_cell = {
                "cell_type": "markdown",
                "metadata": {},
                "source": [f"## {operation.replace('_', ' ').title()}"]
            }
            notebook["cells"].append(markdown_cell)
            
            code = self._generate_operation_code(operation, params, df_name)
            code_cell = {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": code.split('\n')
            }
            notebook["cells"].append(code_cell)
        
        return notebook
    
    def _generate_operation_code(self, operation: str, params: Dict, df_name: str) -> str:
        """Generate code for a single operation"""
        if operation == 'remove_duplicates':
            return f"{df_name} = {df_name}.drop_duplicates()\nprint(f'Shape after removing duplicates: {{{df_name}.shape}}')"
        
        elif operation == 'handle_missing':
            lines = []
            for col, method in params.items():
                if method == 'mean':
                    lines.append(f"{df_name}['{col}'].fillna({df_name}['{col}'].mean(), inplace=True)")
                elif method == 'median':
                    lines.append(f"{df_name}['{col}'].fillna({df_name}['{col}'].median(), inplace=True)")
            return '\n'.join(lines)
        
        return f"# {operation} operation"
    
    def clear_operations(self):
        """Clear all stored operations"""
        self.operations.clear()


class LearningEngine:
    """Learns from user preferences and improves suggestions"""
    
    def __init__(self):
        self.user_preferences = {
            'missing_value_strategies': {},
            'outlier_strategies': {},
            'type_conversions': {},
            'approved_operations': [],
            'rejected_operations': [],
            'modifications': []
        }
        self.feedback_history = []
    
    def record_approval(self, operation: str, parameters: Dict[str, Any], 
                       column: str = None, data_type: str = None):
        """Record that user approved an operation"""
        feedback = {
            'timestamp': datetime.now().isoformat(),
            'action': 'approved',
            'operation': operation,
            'parameters': parameters,
            'column': column,
            'data_type': data_type
        }
        
        self.feedback_history.append(feedback)
        self.user_preferences['approved_operations'].append(feedback)
        
        # Learn patterns
        if operation == 'handle_missing' and column:
            for col, method in parameters.items():
                if col not in self.user_preferences['missing_value_strategies']:
                    self.user_preferences['missing_value_strategies'][col] = []
                self.user_preferences['missing_value_strategies'][col].append(method)
        
        elif operation == 'handle_outliers' and column:
            for col, method in parameters.items():
                if col not in self.user_preferences['outlier_strategies']:
                    self.user_preferences['outlier_strategies'][col] = []
                self.user_preferences['outlier_strategies'][col].append(method)
    
    def record_rejection(self, operation: str, parameters: Dict[str, Any],
                        reason: str = None):
        """Record that user rejected an operation"""
        feedback = {
            'timestamp': datetime.now().isoformat(),
            'action': 'rejected',
            'operation': operation,
            'parameters': parameters,
            'reason': reason
        }
        
        self.feedback_history.append(feedback)
        self.user_preferences['rejected_operations'].append(feedback)
    
    def record_modification(self, original_operation: str, original_params: Dict,
                          modified_params: Dict):
        """Record that user modified a suggestion"""
        feedback = {
            'timestamp': datetime.now().isoformat(),
            'action': 'modified',
            'operation': original_operation,
            'original_parameters': original_params,
            'modified_parameters': modified_params
        }
        
        self.feedback_history.append(feedback)
        self.user_preferences['modifications'].append(feedback)
    
    def get_preferred_strategy(self, operation: str, column: str = None,
                              data_type: str = None) -> Optional[str]:
        """Get user's preferred strategy based on learning"""
        if operation == 'handle_missing':
            if column in self.user_preferences['missing_value_strategies']:
                strategies = self.user_preferences['missing_value_strategies'][column]
                # Return most common strategy
                if strategies:
                    return max(set(strategies), key=strategies.count)
        
        elif operation == 'handle_outliers':
            if column in self.user_preferences['outlier_strategies']:
                strategies = self.user_preferences['outlier_strategies'][column]
                if strategies:
                    return max(set(strategies), key=strategies.count)
        
        return None
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """Get insights from learning"""
        total_feedback = len(self.feedback_history)
        approvals = len(self.user_preferences['approved_operations'])
        rejections = len(self.user_preferences['rejected_operations'])
        modifications = len(self.user_preferences['modifications'])
        
        approval_rate = (approvals / total_feedback * 100) if total_feedback > 0 else 0
        
        return {
            'total_interactions': total_feedback,
            'approvals': approvals,
            'rejections': rejections,
            'modifications': modifications,
            'approval_rate': approval_rate,
            'learned_patterns': {
                'missing_value_preferences': len(self.user_preferences['missing_value_strategies']),
                'outlier_preferences': len(self.user_preferences['outlier_strategies'])
            }
        }
    
    def export_preferences(self) -> str:
        """Export preferences as JSON"""
        return json.dumps(self.user_preferences, indent=2)
    
    def import_preferences(self, preferences_json: str):
        """Import preferences from JSON"""
        try:
            self.user_preferences = json.loads(preferences_json)
        except json.JSONDecodeError:
            raise ValueError("Invalid preferences JSON")
    
    def reset_learning(self):
        """Reset all learning data"""
        self.user_preferences = {
            'missing_value_strategies': {},
            'outlier_strategies': {},
            'type_conversions': {},
            'approved_operations': [],
            'rejected_operations': [],
            'modifications': []
        }
        self.feedback_history = []


class RecipeManager:
    """Manages cleaning recipes (templates)"""
    
    def __init__(self):
        self.recipes = {}
    
    def save_recipe(self, name: str, operations: List[Dict], 
                   description: str = "", tags: List[str] = None) -> Dict:
        """Save a cleaning recipe"""
        recipe = {
            'name': name,
            'description': description,
            'created_at': datetime.now().isoformat(),
            'operations': operations,
            'tags': tags or [],
            'usage_count': 0
        }
        
        self.recipes[name] = recipe
        return recipe
    
    def load_recipe(self, name: str) -> Optional[Dict]:
        """Load a recipe"""
        if name in self.recipes:
            self.recipes[name]['usage_count'] += 1
            return self.recipes[name]
        return None
    
    def list_recipes(self) -> List[Dict]:
        """List all recipes"""
        return [
            {
                'name': name,
                'description': recipe['description'],
                'created_at': recipe['created_at'],
                'operations_count': len(recipe['operations']),
                'tags': recipe['tags'],
                'usage_count': recipe['usage_count']
            }
            for name, recipe in self.recipes.items()
        ]
    
    def delete_recipe(self, name: str) -> bool:
        """Delete a recipe"""
        if name in self.recipes:
            del self.recipes[name]
            return True
        return False
    
    def search_recipes(self, query: str = None, tags: List[str] = None) -> List[Dict]:
        """Search recipes by name or tags"""
        results = []
        
        for name, recipe in self.recipes.items():
            match = False
            
            if query and query.lower() in name.lower():
                match = True
            
            if tags and any(tag in recipe['tags'] for tag in tags):
                match = True
            
            if match or (query is None and tags is None):
                results.append({
                    'name': name,
                    'description': recipe['description'],
                    'operations_count': len(recipe['operations'])
                })
        
        return results
    
    def export_recipe(self, name: str) -> Optional[str]:
        """Export recipe as JSON"""
        if name in self.recipes:
            return json.dumps(self.recipes[name], indent=2)
        return None
    
    def import_recipe(self, recipe_json: str) -> bool:
        """Import recipe from JSON"""
        try:
            recipe = json.loads(recipe_json)
            name = recipe.get('name', f"imported_recipe_{len(self.recipes)}")
            self.recipes[name] = recipe
            return True
        except json.JSONDecodeError:
            return False
    
    def get_popular_recipes(self, limit: int = 5) -> List[Dict]:
        """Get most used recipes"""
        sorted_recipes = sorted(
            self.recipes.items(),
            key=lambda x: x[1]['usage_count'],
            reverse=True
        )
        
        return [
            {
                'name': name,
                'description': recipe['description'],
                'usage_count': recipe['usage_count']
            }
            for name, recipe in sorted_recipes[:limit]
        ]


# Pre-built industry templates
INDUSTRY_TEMPLATES = {
    'financial_data': {
        'name': 'Financial Data Cleaning',
        'description': 'Standard cleaning for financial datasets',
        'operations': [
            {'operation': 'remove_duplicates', 'parameters': {}},
            {'operation': 'handle_missing', 'parameters': {
                'amount': 'median',
                'date': 'drop',
                'category': 'mode'
            }},
            {'operation': 'handle_outliers', 'parameters': {
                'amount': 'cap',
                'price': 'cap'
            }}
        ],
        'tags': ['finance', 'banking', 'transactions']
    },
    'customer_data': {
        'name': 'Customer Data Cleaning',
        'description': 'Standard cleaning for customer/CRM data',
        'operations': [
            {'operation': 'remove_duplicates', 'parameters': {}},
            {'operation': 'normalize_text', 'parameters': ['name', 'email', 'address']},
            {'operation': 'handle_missing', 'parameters': {
                'email': 'constant:no-email@domain.com',
                'phone': 'mode',
                'age': 'median'
            }}
        ],
        'tags': ['customer', 'crm', 'contact']
    },
    'ecommerce_data': {
        'name': 'E-commerce Product Cleaning',
        'description': 'Cleaning for product catalogs',
        'operations': [
            {'operation': 'remove_duplicates', 'parameters': {}},
            {'operation': 'normalize_text', 'parameters': ['product_name', 'description', 'category']},
            {'operation': 'handle_missing', 'parameters': {
                'price': 'median',
                'stock': 'constant:0',
                'description': 'constant:No description'
            }},
            {'operation': 'handle_outliers', 'parameters': {
                'price': 'cap'
            }}
        ],
        'tags': ['ecommerce', 'products', 'retail']
    }
}