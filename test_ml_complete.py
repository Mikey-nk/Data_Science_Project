"""
Comprehensive Test Suite for Complete ML Implementation
Tests all features to ensure production readiness
"""

import pandas as pd
import numpy as np
from predictive_modeling import (
    PredictionPipeline, ModelBuilder, ModelType, ProblemType, ModelConfig
)
from sklearn.datasets import make_classification, make_regression
import os
import time


def print_section(title):
    """Print formatted section header"""
    print("\n" + "="*60)
    print(f"  {title}")
    print("="*60)


def test_classification():
    """Test classification pipeline"""
    print_section("ğŸ¯ Testing Classification")
    
    # Create sample data
    X, y = make_classification(
        n_samples=1000, n_features=10, n_informative=7,
        n_classes=2, random_state=42
    )
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
    df['target'] = y
    
    # Add some categorical and missing data
    df['category'] = np.random.choice(['A', 'B', 'C'], size=len(df))
    df.loc[np.random.choice(df.index, 50), 'feature_0'] = np.nan
    
    print(f"âœ… Created dataset: {df.shape}")
    print(f"   - Target classes: {df['target'].nunique()}")
    print(f"   - Missing values: {df.isnull().sum().sum()}")
    
    # Create pipeline
    pipeline = PredictionPipeline(df)
    
    # Test 1: Analyze readiness
    print("\nğŸ“Š Test 1: Analyze Prediction Readiness")
    analysis = pipeline.analyze_prediction_readiness()
    print(f"   âœ… Ready: {analysis['ready']}")
    print(f"   âœ… Quality score: {analysis['data_quality_score']:.1f}")
    print(f"   âœ… Potential targets: {len(analysis['potential_targets'])}")
    assert analysis['ready'], "Data should be ready!"
    
    # Test 2: Train basic model
    print("\nğŸ¤– Test 2: Train Random Forest Classifier")
    start = time.time()
    results = pipeline.train_model(
        target_column='target',
        model_type=ModelType.RANDOM_FOREST_CLASSIFIER,
        tune_hyperparameters=False
    )
    elapsed = time.time() - start
    
    print(f"   âœ… Training completed in {elapsed:.2f}s")
    print(f"   âœ… Accuracy: {results.metrics['accuracy']:.4f}")
    print(f"   âœ… Precision: {results.metrics['precision']:.4f}")
    print(f"   âœ… Recall: {results.metrics['recall']:.4f}")
    print(f"   âœ… F1 Score: {results.metrics['f1_score']:.4f}")
    
    assert results.metrics['accuracy'] > 0.7, "Accuracy too low!"
    assert results.feature_importance is not None, "Feature importance missing!"
    
    # Test 3: Cross-validation scores
    print("\nğŸ“ˆ Test 3: Cross-Validation")
    print(f"   âœ… CV Mean Accuracy: {results.cv_scores['mean_accuracy']:.4f}")
    print(f"   âœ… CV Std: {results.cv_scores['std_accuracy']:.4f}")
    assert results.cv_scores['mean_accuracy'] > 0.7, "CV accuracy too low!"
    
    # Test 4: Feature importance
    print("\nğŸ¯ Test 4: Feature Importance")
    top_features = sorted(
        results.feature_importance.items(),
        key=lambda x: x[1],
        reverse=True
    )[:3]
    for feat, imp in top_features:
        print(f"   âœ… {feat}: {imp:.4f}")
    
    # Test 5: Model persistence
    print("\nğŸ’¾ Test 5: Model Persistence")
    model_key = list(pipeline.trained_models.keys())[0]
    filepath = 'test_model_classification.pkl'
    saved_path = pipeline.export_model(model_key, filepath)
    print(f"   âœ… Model saved to: {saved_path}")
    assert os.path.exists(filepath), "Model file not created!"
    
    # Test 6: Load model
    print("\nğŸ“‚ Test 6: Load Model")
    new_pipeline = PredictionPipeline(df)
    new_pipeline.load_model(filepath)
    print(f"   âœ… Model loaded successfully")
    assert len(new_pipeline.trained_models) > 0, "Model not loaded!"
    
    # Test 7: Predictions on new data
    print("\nğŸ”® Test 7: Predictions on New Data")
    new_data = df.head(10).drop('target', axis=1)
    predictions_df = new_pipeline.predict_new_data(model_key, new_data)
    print(f"   âœ… Predictions shape: {predictions_df.shape}")
    print(f"   âœ… Has 'prediction' column: {'prediction' in predictions_df.columns}")
    print(f"   âœ… Sample predictions: {predictions_df['prediction'].head(3).tolist()}")
    
    # Clean up
    if os.path.exists(filepath):
        os.remove(filepath)
    
    print("\nâœ¨ Classification tests PASSED!")
    return True


def test_regression():
    """Test regression pipeline"""
    print_section("ğŸ“ˆ Testing Regression")
    
    # Create sample data
    X, y = make_regression(
        n_samples=1000, n_features=10, noise=10, random_state=42
    )
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
    df['target'] = y
    
    print(f"âœ… Created dataset: {df.shape}")
    print(f"   - Target range: [{df['target'].min():.2f}, {df['target'].max():.2f}]")
    
    pipeline = PredictionPipeline(df)
    
    # Test 1: Train regression model
    print("\nğŸ¤– Test 1: Train Random Forest Regressor")
    results = pipeline.train_model(
        target_column='target',
        model_type=ModelType.RANDOM_FOREST_REGRESSOR
    )
    
    print(f"   âœ… RÂ² Score: {results.metrics['r2_score']:.4f}")
    print(f"   âœ… RMSE: {results.metrics['rmse']:.2f}")
    print(f"   âœ… MAE: {results.metrics['mae']:.2f}")
    
    assert results.metrics['r2_score'] > 0.5, "RÂ² too low!"
    
    # Test 2: Cross-validation
    print("\nğŸ“Š Test 2: Cross-Validation")
    print(f"   âœ… CV Mean RÂ²: {results.cv_scores['mean_r2']:.4f}")
    print(f"   âœ… CV Std RÂ²: {results.cv_scores['std_r2']:.4f}")
    
    # Test 3: Predictions
    print("\nğŸ”® Test 3: Predictions")
    model_key = list(pipeline.trained_models.keys())[0]
    new_data = df.head(5).drop('target', axis=1)
    predictions_df = pipeline.predict_new_data(model_key, new_data)
    
    print(f"   âœ… Predictions: {predictions_df['prediction'].values[:3]}")
    
    print("\nâœ¨ Regression tests PASSED!")
    return True


def test_hyperparameter_tuning():
    """Test hyperparameter tuning"""
    print_section("ğŸ”§ Testing Hyperparameter Tuning")
    
    # Create small dataset for faster tuning
    X, y = make_classification(
        n_samples=500, n_features=5, n_informative=3,
        n_classes=2, random_state=42
    )
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(5)])
    df['target'] = y
    
    pipeline = PredictionPipeline(df)
    
    print("\nğŸ” Training with hyperparameter tuning...")
    print("   (This may take 30-60 seconds)")
    
    start = time.time()
    results = pipeline.train_model(
        target_column='target',
        model_type=ModelType.RANDOM_FOREST_CLASSIFIER,
        tune_hyperparameters=True
    )
    elapsed = time.time() - start
    
    print(f"\n   âœ… Tuning completed in {elapsed:.2f}s")
    print(f"   âœ… Best params: {results.best_params}")
    print(f"   âœ… Accuracy: {results.metrics['accuracy']:.4f}")
    
    assert results.best_params is not None, "Best params not found!"
    assert 'n_estimators' in results.best_params, "n_estimators not tuned!"
    
    print("\nâœ¨ Hyperparameter tuning tests PASSED!")
    return True


def test_model_comparison():
    """Test model comparison"""
    print_section("ğŸ“Š Testing Model Comparison")
    
    # Create dataset
    X, y = make_classification(
        n_samples=500, n_features=8, n_informative=5,
        n_classes=2, random_state=42
    )
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(8)])
    df['target'] = y
    
    pipeline = PredictionPipeline(df)
    
    # Train multiple models
    models = [
        ModelType.LOGISTIC_REGRESSION,
        ModelType.RANDOM_FOREST_CLASSIFIER,
        ModelType.DECISION_TREE_CLASSIFIER
    ]
    
    print("\nğŸ¤– Training multiple models...")
    for model_type in models:
        print(f"   Training {model_type.value}...")
        results = pipeline.train_model(
            target_column='target',
            model_type=model_type
        )
        print(f"   âœ… Accuracy: {results.metrics['accuracy']:.4f}")
    
    # Compare models
    print("\nğŸ“ˆ Comparing models...")
    comparison = pipeline.compare_models(list(pipeline.trained_models.keys()))
    print(comparison.to_string(index=False))
    
    # Get best model
    best = pipeline.get_best_model(metric='accuracy')
    print(f"\nğŸ† Best model: {best['results'].model_type}")
    print(f"   Accuracy: {best['results'].metrics['accuracy']:.4f}")
    
    assert len(pipeline.trained_models) == 3, "Not all models trained!"
    assert best is not None, "Best model not found!"
    
    print("\nâœ¨ Model comparison tests PASSED!")
    return True


def test_recommendations():
    """Test model recommendations"""
    print_section("ğŸ’¡ Testing Model Recommendations")
    
    # Create dataset with proper parameters to avoid sklearn error
    X, y = make_classification(
        n_samples=1000, 
        n_features=10, 
        n_classes=3, 
        n_informative=8,  # Increased to avoid error
        n_redundant=2,
        random_state=42
    )
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
    df['target'] = y
    
    pipeline = PredictionPipeline(df)
    
    # Get recommendations
    recommendations = pipeline.get_model_recommendations('target')
    
    print(f"\nğŸ“‹ Got {len(recommendations)} recommendations:")
    for rec in recommendations:
        print(f"\n   ğŸ¤– {rec['model'].value}")
        print(f"      Best for: {rec['best_for']}")
        print(f"      Complexity: {rec['complexity']}")
        print(f"      Speed: {rec['training_speed']}")
        print(f"      Pros: {', '.join(rec['pros'][:2])}")
    
    assert len(recommendations) >= 3, "Not enough recommendations!"
    
    print("\nâœ¨ Recommendations tests PASSED!")
    return True


def test_edge_cases():
    """Test edge cases and error handling"""
    print_section("ğŸ› Testing Edge Cases")
    
    # Test 1: Very small dataset
    print("\nğŸ“ Test 1: Small dataset (50 rows)")
    X, y = make_classification(n_samples=50, n_features=5, random_state=42)
    df_small = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(5)])
    df_small['target'] = y
    
    pipeline_small = PredictionPipeline(df_small)
    analysis = pipeline_small.analyze_prediction_readiness()
    print(f"   âœ… Analysis completed")
    print(f"   âœ… Ready: {analysis['ready']}")
    
    # Should still train but with warning
    results_small = pipeline_small.train_model(
        target_column='target',
        model_type=ModelType.LOGISTIC_REGRESSION
    )
    print(f"   âœ… Model trained: Accuracy {results_small.metrics['accuracy']:.4f}")
    
    # Test 2: Missing target column
    print("\nâŒ Test 2: Missing target column (should handle gracefully)")
    try:
        pipeline_small.train_model(
            target_column='nonexistent',
            model_type=ModelType.LOGISTIC_REGRESSION
        )
        print("   âŒ Should have raised error!")
        return False
    except ValueError as e:
        print(f"   âœ… Correctly raised error: {str(e)[:50]}...")
    
    # Test 3: Categorical target
    print("\nğŸ“ Test 3: Categorical target (text)")
    df_cat = df_small.copy()
    df_cat['target_cat'] = df_cat['target'].map({0: 'ClassA', 1: 'ClassB'})
    
    pipeline_cat = PredictionPipeline(df_cat)
    results_cat = pipeline_cat.train_model(
        target_column='target_cat',
        model_type=ModelType.RANDOM_FOREST_CLASSIFIER
    )
    print(f"   âœ… Handled categorical target")
    print(f"   âœ… Accuracy: {results_cat.metrics['accuracy']:.4f}")
    
    print("\nâœ¨ Edge case tests PASSED!")
    return True


def run_all_tests():
    """Run all tests"""
    print("\n" + "ğŸ§ª"*30)
    print("  COMPLETE ML IMPLEMENTATION TEST SUITE")
    print("ğŸ§ª"*30)
    
    tests = [
        ("Classification", test_classification),
        ("Regression", test_regression),
        ("Hyperparameter Tuning", test_hyperparameter_tuning),
        ("Model Comparison", test_model_comparison),
        ("Recommendations", test_recommendations),
        ("Edge Cases", test_edge_cases)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        try:
            success = test_func()
            results.append((name, success))
        except Exception as e:
            print(f"\nâŒ Test '{name}' FAILED with error:")
            print(f"   {str(e)}")
            results.append((name, False))
    
    elapsed = time.time() - start_time
    
    # Summary
    print_section("ğŸ“Š TEST SUMMARY")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status}  {name}")
    
    print(f"\n{'='*60}")
    print(f"Results: {passed}/{total} tests passed")
    print(f"Time: {elapsed:.2f} seconds")
    print(f"{'='*60}")
    
    if passed == total:
        print("\nğŸ‰ ALL TESTS PASSED! ML implementation is production-ready!")
        print("\nâœ¨ Features verified:")
        print("   âœ… Real sklearn models (not mocks)")
        print("   âœ… Actual training and predictions")
        print("   âœ… Hyperparameter tuning")
        print("   âœ… Cross-validation")
        print("   âœ… Model persistence (save/load)")
        print("   âœ… Feature importance")
        print("   âœ… Model comparison")
        print("   âœ… Edge case handling")
        print("\nğŸš€ Ready for production deployment!")
        return True
    else:
        print(f"\nâŒ {total - passed} test(s) failed. Check errors above.")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)